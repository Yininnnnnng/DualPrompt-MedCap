{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install open_clip_torch\n",
        "!pip install einops_exts"
      ],
      "metadata": {
        "id": "Ioqw_K-Jm7iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy scispacy\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_lg-0.5.1.tar.gz"
      ],
      "metadata": {
        "id": "tOHpT7RonYlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "from scispacy.linking import EntityLinker\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import traceback\n",
        "import json\n",
        "import io\n",
        "import pandas as pd\n",
        "import os\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VnVbgUhxjLuj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e41723e-7e1e-400d-82e8-bcf7bbb213ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalImageCaptionEvaluator:\n",
        "    def __init__(self):\n",
        "        print(\"Initializing evaluator...\")\n",
        "\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # 1. BiomedCLIP\n",
        "        print(\"Loading BiomedCLIP...\")\n",
        "        self.model, self.preprocess = open_clip.create_model_from_pretrained(\n",
        "            'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
        "        )\n",
        "        self.tokenizer = open_clip.get_tokenizer(\n",
        "            'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # 2. NLP\n",
        "        print(\"Loading Medical NLP model...\")\n",
        "        self.nlp = spacy.load(\"en_core_sci_lg\")\n",
        "        self.nlp.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\"})\n",
        "\n",
        "        # 3. dictionary\n",
        "        self.medical_patterns = {\n",
        "            'modality_terms': [\n",
        "                'CT scan', 'MRI', 'X-ray', 'radiograph', 'MRI scan',\n",
        "                'chest X-ray', 'abdominal CT', 'ultrasound', 'PET scan'\n",
        "            ],\n",
        "            'anatomy_terms': [\n",
        "                'brain', 'lung', 'heart', 'liver', 'kidney', 'spine',\n",
        "                'rib cage', 'skull', 'cerebral cortex', 'ventricle',\n",
        "                'pancreas', 'spleen', 'intestine', 'bladder', 'chest'\n",
        "            ],\n",
        "            'location_terms': [\n",
        "                'left', 'right', 'anterior', 'posterior', 'superior',\n",
        "                'inferior', 'lateral', 'medial', 'central', 'upper', 'lower',\n",
        "                'bilateral', 'unilateral'\n",
        "            ],\n",
        "            'finding_terms': [\n",
        "                'normal', 'abnormal', 'mass', 'lesion', 'fracture',\n",
        "                'pneumonia', 'effusion', 'enlargement', 'tumor',\n",
        "                'inflammation', 'infection', 'opacity', 'nodule'\n",
        "            ],\n",
        "            'measurement_terms': [\n",
        "                'mm', 'cm', 'size', 'diameter', 'volume', 'density',\n",
        "                'measurement', 'dimension'\n",
        "            ],\n",
        "            'comparison_terms': [\n",
        "                'increased', 'decreased', 'unchanged', 'stable',\n",
        "                'improved', 'worsened', 'compared to'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        print(\"All models loaded successfully!\")\n",
        "\n",
        "    def evaluate_medical_quality(self, caption):\n",
        "        \"\"\"quality\"\"\"\n",
        "        try:\n",
        "            doc = self.nlp(caption)\n",
        "\n",
        "\n",
        "            term_scores = {\n",
        "                'modality': sum(term.lower() in caption.lower() for term in self.medical_patterns['modality_terms']),\n",
        "                'anatomy': sum(term.lower() in caption.lower() for term in self.medical_patterns['anatomy_terms']),\n",
        "                'location': sum(term.lower() in caption.lower() for term in self.medical_patterns['location_terms']),\n",
        "                'finding': sum(term.lower() in caption.lower() for term in self.medical_patterns['finding_terms']),\n",
        "                'measurement': sum(term.lower() in caption.lower() for term in self.medical_patterns['measurement_terms']),\n",
        "                'comparison': sum(term.lower() in caption.lower() for term in self.medical_patterns['comparison_terms'])\n",
        "            }\n",
        "\n",
        "\n",
        "            medical_entities = []\n",
        "            for ent in doc.ents:\n",
        "                if ent._.kb_ents:\n",
        "                    medical_entities.append(ent.text)\n",
        "\n",
        "\n",
        "            word_count = len(caption.split())\n",
        "            scores = {\n",
        "                'term_usage': min(sum(term_scores.values()) / max(word_count, 1), 1.0),\n",
        "                'entity_density': len(medical_entities) / max(word_count, 1),\n",
        "                'term_diversity': len(set(medical_entities)) / max(len(medical_entities), 1)\n",
        "            }\n",
        "\n",
        "\n",
        "            medical_quality_score = (\n",
        "                0.4 * scores['term_usage'] +\n",
        "                0.3 * scores['entity_density'] +\n",
        "                0.3 * scores['term_diversity']\n",
        "            )\n",
        "\n",
        "            return medical_quality_score, scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_medical_quality: {e}\")\n",
        "            return 0.0, {'term_usage': 0, 'entity_density': 0, 'term_diversity': 0}\n",
        "\n",
        "    def evaluate_clinical_accuracy(self, caption):\n",
        "        \"\"\"acc\"\"\"\n",
        "        try:\n",
        "            # dignostic\n",
        "            has_finding = any(term.lower() in caption.lower() for term in self.medical_patterns['finding_terms'])\n",
        "            # anatomy\n",
        "            has_location = any(term.lower() in caption.lower() for term in self.medical_patterns['location_terms'])\n",
        "            has_anatomy = any(term.lower() in caption.lower() for term in self.medical_patterns['anatomy_terms'])\n",
        "\n",
        "            has_measurements = any(term.lower() in caption.lower() for term in self.medical_patterns['measurement_terms'])\n",
        "\n",
        "            has_comparison = any(term.lower() in caption.lower() for term in self.medical_patterns['comparison_terms'])\n",
        "\n",
        "            scores = {\n",
        "                'findings': float(has_finding),\n",
        "                'location': float(has_location and has_anatomy),\n",
        "                'measurement': float(has_measurements),\n",
        "                'comparison': float(has_comparison)\n",
        "            }\n",
        "\n",
        "            clinical_accuracy_score = (\n",
        "                0.375 * scores['findings'] +\n",
        "                0.25 * scores['location'] +\n",
        "                0.25 * scores['measurement'] +\n",
        "                0.125 * scores['comparison']\n",
        "            )\n",
        "\n",
        "            return clinical_accuracy_score, scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_clinical_accuracy: {e}\")\n",
        "            return 0.0, {'findings': 0, 'location': 0, 'measurement': 0, 'comparison': 0}\n",
        "\n",
        "    def evaluate_report_structure(self, caption):\n",
        "        \"\"\"structure\"\"\"\n",
        "        try:\n",
        "\n",
        "            has_modality = any(term.lower() in caption.lower() for term in self.medical_patterns['modality_terms'])\n",
        "            has_anatomy = any(term.lower() in caption.lower() for term in self.medical_patterns['anatomy_terms'])\n",
        "            basic_structure_score = float(has_modality and has_anatomy)\n",
        "\n",
        "\n",
        "            sentences = [s.strip() for s in caption.split('.') if s.strip()]\n",
        "            has_multiple_sentences = len(sentences) >= 2\n",
        "            completeness_score = float(has_multiple_sentences)\n",
        "\n",
        "\n",
        "            has_conclusion = bool(re.search(r'(suggest|indicate|represent|conclude|impression)', caption.lower()))\n",
        "            logical_flow_score = float(has_conclusion)\n",
        "\n",
        "            structure_score = (\n",
        "                1/3 * basic_structure_score +\n",
        "                1/3 * completeness_score +\n",
        "                1/3 * logical_flow_score\n",
        "            )\n",
        "\n",
        "            scores = {\n",
        "                'basic_structure': basic_structure_score,\n",
        "                'completeness': completeness_score,\n",
        "                'logical_flow': logical_flow_score\n",
        "            }\n",
        "\n",
        "            return structure_score, scores\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_report_structure: {e}\")\n",
        "            return 0.0, {'basic_structure': 0, 'completeness': 0, 'logical_flow': 0}\n",
        "\n",
        "    def compute_similarities(self, image, caption, question):\n",
        "        \"\"\"similarity\"\"\"\n",
        "        try:\n",
        "            image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "            caption_tokens = self.tokenizer(caption).to(self.device)\n",
        "            question_tokens = self.tokenizer(question).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.model.encode_image(image_input)\n",
        "                caption_features = self.model.encode_text(caption_tokens)\n",
        "                question_features = self.model.encode_text(question_tokens)\n",
        "\n",
        "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                caption_features /= caption_features.norm(dim=-1, keepdim=True)\n",
        "                question_features /= question_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                image_text_score = torch.cosine_similarity(image_features, caption_features).item()\n",
        "                question_text_score = torch.cosine_similarity(question_features, caption_features).item()\n",
        "\n",
        "            return image_text_score, question_text_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in compute_similarities: {e}\")\n",
        "            return 0.0, 0.0\n",
        "\n",
        "\n",
        "    def compute_final_score(self, image_text_score, question_text_score, medical_quality_score,\n",
        "                          clinical_accuracy_score, structure_score):\n",
        "        \"\"\"\n",
        "new weight\n",
        "        \"\"\"\n",
        "        try:\n",
        "\n",
        "            relevance_score = (\n",
        "                0.25 * image_text_score +\n",
        "                0.25 * question_text_score\n",
        "            )\n",
        "\n",
        "            )\n",
        "            quality_score = (\n",
        "                (0.5/3) * medical_quality_score +\n",
        "                (0.5/3) * clinical_accuracy_score +\n",
        "                (0.5/3) * structure_score\n",
        "            )\n",
        "\n",
        "\n",
        "            final_score = relevance_score + quality_score\n",
        "\n",
        "\n",
        "            if image_text_score < 0.2:\n",
        "                final_score *= 0.5\n",
        "            if question_text_score < 0.2:\n",
        "                final_score *= 0.5\n",
        "\n",
        "            return final_score, {\n",
        "                'relevance_score': relevance_score,\n",
        "                'quality_score': quality_score\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in compute_final_score: {e}\")\n",
        "            return 0.0, {'relevance_score': 0, 'quality_score': 0}\n",
        "\n",
        "\n",
        "    def evaluate_caption(self, image, caption, question):\n",
        "        \"\"\"combine\"\"\"\n",
        "        try:\n",
        "\n",
        "            medical_quality_score, medical_quality_details = self.evaluate_medical_quality(caption)\n",
        "\n",
        "\n",
        "            clinical_accuracy_score, clinical_accuracy_details = self.evaluate_clinical_accuracy(caption)\n",
        "\n",
        "\n",
        "            structure_score, structure_details = self.evaluate_report_structure(caption)\n",
        "\n",
        "\n",
        "            image_text_score, question_text_score = self.compute_similarities(image, caption, question)\n",
        "\n",
        "\n",
        "            final_score, score_details = self.compute_final_score(\n",
        "                image_text_score,\n",
        "                question_text_score,\n",
        "                medical_quality_score,\n",
        "                clinical_accuracy_score,\n",
        "                structure_score\n",
        "            )\n",
        "\n",
        "\n",
        "            return {\n",
        "                'final_score': final_score,\n",
        "                'medical_quality': medical_quality_score,\n",
        "                'clinical_accuracy': clinical_accuracy_score,\n",
        "                'structure': structure_score,\n",
        "                'image_similarity': image_text_score,\n",
        "                'question_similarity': question_text_score,\n",
        "                'caption': caption,\n",
        "                'question': question,\n",
        "                'detailed_scores': {\n",
        "                    'medical_quality': medical_quality_details,\n",
        "                    'clinical_accuracy': clinical_accuracy_details,\n",
        "                    'structure': structure_details,\n",
        "                    'score_components': score_details\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in evaluate_caption: {e}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "Kr_I_envjLw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_case(case_data, img_path=None, case_type=\"\", evaluator=None):\n",
        "    \"\"\"show cases\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n{'='*20} {case_type} Case {'='*20}\")\n",
        "\n",
        "\n",
        "        print(f\"Index/QID: {case_data['index']}\")\n",
        "        print(f\"Image Name: {case_data['img_name']}\")\n",
        "        print(f\"Modality: {case_data.get('modality', 'Unknown')}\")\n",
        "        print(f\"Final Score: {case_data['final_score']:.4f}\")\n",
        "\n",
        "\n",
        "        print(f\"\\nQuestion: {case_data['question']}\")\n",
        "        if 'original_answer' in case_data:\n",
        "            print(f\"Original Answer: {case_data['original_answer']}\")\n",
        "        print(f\"\\nGenerated Caption: {case_data['caption']}\")\n",
        "\n",
        "\n",
        "        print(\"\\n----- Component Scores -----\")\n",
        "        print(f\"Medical Quality: {case_data['medical_quality']:.4f}\")\n",
        "        print(f\"Clinical Accuracy: {case_data['clinical_accuracy']:.4f}\")\n",
        "        print(f\"Structure: {case_data['structure']:.4f}\")\n",
        "        print(f\"Image Similarity: {case_data['image_similarity']:.4f}\")\n",
        "        print(f\"Question Similarity: {case_data['question_similarity']:.4f}\")\n",
        "\n",
        "\n",
        "        if 'detailed_scores' in case_data:\n",
        "            print(\"\\n----- Detailed Analysis -----\")\n",
        "\n",
        "\n",
        "            if 'medical_quality' in case_data['detailed_scores']:\n",
        "                med_quality = case_data['detailed_scores']['medical_quality']\n",
        "                print(\"\\nMedical Quality Analysis:\")\n",
        "                print(f\"  Term Usage: {med_quality.get('term_usage', 0):.4f}\")\n",
        "                print(f\"  Entity Density: {med_quality.get('entity_density', 0):.4f}\")\n",
        "                print(f\"  Term Diversity: {med_quality.get('term_diversity', 0):.4f}\")\n",
        "\n",
        "\n",
        "            if 'clinical_accuracy' in case_data['detailed_scores']:\n",
        "                clin_acc = case_data['detailed_scores']['clinical_accuracy']\n",
        "                print(\"\\nClinical Accuracy Analysis:\")\n",
        "                print(f\"  Findings: {clin_acc.get('findings', 0):.4f}\")\n",
        "                print(f\"  Location: {clin_acc.get('location', 0):.4f}\")\n",
        "                print(f\"  Measurement: {clin_acc.get('measurement', 0):.4f}\")\n",
        "                print(f\"  Comparison: {clin_acc.get('comparison', 0):.4f}\")\n",
        "\n",
        "\n",
        "            if 'structure' in case_data['detailed_scores']:\n",
        "                structure = case_data['detailed_scores']['structure']\n",
        "                print(\"\\nReport Structure Analysis:\")\n",
        "                print(f\"  Basic Structure: {structure.get('basic_structure', 0):.4f}\")\n",
        "                print(f\"  Completeness: {structure.get('completeness', 0):.4f}\")\n",
        "                print(f\"  Logical Flow: {structure.get('logical_flow', 0):.4f}\")\n",
        "\n",
        "\n",
        "            if 'score_components' in case_data['detailed_scores']:\n",
        "                components = case_data['detailed_scores']['score_components']\n",
        "                print(\"\\nScore Components:\")\n",
        "                print(f\"  Relevance Score: {components.get('relevance_score', 0):.4f}\")\n",
        "                print(f\"  Quality Score: {components.get('quality_score', 0):.4f}\")\n",
        "\n",
        "\n",
        "        if evaluator is not None:\n",
        "            print(\"\\n----- Keyword Analysis -----\")\n",
        "            caption_lower = case_data['caption'].lower()\n",
        "\n",
        "\n",
        "            print(\"\\nDetected Medical Terms:\")\n",
        "            for category, terms in [\n",
        "                (\"Modality Terms\", evaluator.medical_patterns['modality_terms']),\n",
        "                (\"Anatomy Terms\", evaluator.medical_patterns['anatomy_terms']),\n",
        "                (\"Location Terms\", evaluator.medical_patterns['location_terms']),\n",
        "                (\"Finding Terms\", evaluator.medical_patterns['finding_terms']),\n",
        "                (\"Measurement Terms\", evaluator.medical_patterns['measurement_terms']),\n",
        "                (\"Comparison Terms\", evaluator.medical_patterns['comparison_terms'])\n",
        "            ]:\n",
        "                found_terms = [term for term in terms if term.lower() in caption_lower]\n",
        "                if found_terms:\n",
        "                    print(f\"  {category}: {', '.join(found_terms)}\")\n",
        "\n",
        "\n",
        "            question_keywords = [word.lower() for word in case_data['question'].split() if len(word) > 3]\n",
        "            matching_keywords = [word for word in question_keywords if word in caption_lower]\n",
        "            if matching_keywords:\n",
        "                print(\"\\nQuestion-Related Keywords Found:\")\n",
        "                print(f\"  {', '.join(matching_keywords)}\")\n",
        "\n",
        "\n",
        "        if img_path and os.path.exists(img_path):\n",
        "            print(\"\\nDisplaying Image:\")\n",
        "            print(f\"Image Path: {img_path}\")\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            display(image)\n",
        "        else:\n",
        "            print(\"\\nImage not available or path not found.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error displaying case: {e}\")\n",
        "        print(traceback.format_exc())"
      ],
      "metadata": {
        "id": "tzeFP--6krQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_average_scores(df_results):\n",
        "    \"\"\"show case\"\"\"\n",
        "    print(\"\\n=== Average Scores ===\")\n",
        "    metrics = ['final_score', 'medical_quality', 'clinical_accuracy',\n",
        "              'structure', 'image_similarity', 'question_similarity']\n",
        "\n",
        "    for metric in metrics:\n",
        "        mean_score = df_results[metric].mean()\n",
        "        std_score = df_results[metric].std()\n",
        "        print(f\"{metric}:\")\n",
        "        print(f\"  Mean: {mean_score:.4f}\")\n",
        "        print(f\"  Std: {std_score:.4f}\")"
      ],
      "metadata": {
        "id": "IkXvHxWwjLzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_blip3_enhanced(caption_data, slake_base_dir, evaluator, model_name=\"BLIP3+Prompt\"):\n",
        "    \"\"\"\n",
        "show cases\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nEvaluating {model_name} results...\")\n",
        "        results = []\n",
        "\n",
        "        for item in tqdm(caption_data, desc=f\"Processing {model_name}\"):\n",
        "            try:\n",
        "\n",
        "                qid = item.get('qid', 0)\n",
        "                img_name = item.get('img_name', '')\n",
        "                question = item.get('question', '')\n",
        "                generated_caption = item.get('generated_caption', '')\n",
        "                original_answer = item.get('original_answer', '')\n",
        "\n",
        "                if not generated_caption:\n",
        "                    print(f\"Warning: Missing caption for qid {qid}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "                img_path = os.path.join(slake_base_dir, 'imgs', img_name)\n",
        "\n",
        "\n",
        "                if not os.path.exists(img_path):\n",
        "                    print(f\"Warning: Image not found at {img_path}\")\n",
        "\n",
        "                    alternative_paths = [\n",
        "                        os.path.join(slake_base_dir, 'img', img_name),\n",
        "                        os.path.join(slake_base_dir, 'images', img_name)\n",
        "                    ]\n",
        "                    for alt_path in alternative_paths:\n",
        "                        if os.path.exists(alt_path):\n",
        "                            img_path = alt_path\n",
        "                            print(f\"Found image at alternative path: {img_path}\")\n",
        "                            break\n",
        "                    else:\n",
        "                        print(f\"Error: Could not find image for {img_name}\")\n",
        "                        continue\n",
        "\n",
        "\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "                result = evaluator.evaluate_caption(\n",
        "                    image=image,\n",
        "                    caption=generated_caption,\n",
        "                    question=question\n",
        "                )\n",
        "\n",
        "                if result:\n",
        "\n",
        "                    result['index'] = qid  # qid as index\n",
        "                    result['img_name'] = img_name\n",
        "                    result['img_path'] = img_path\n",
        "                    result['modality'] = item.get('true_modality', '')\n",
        "                    result['original_answer'] = original_answer\n",
        "                    results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing item {qid}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "                continue\n",
        "\n",
        "        # DataFrame\n",
        "        if results:\n",
        "            df_results = pd.DataFrame([\n",
        "                {\n",
        "                    'index': r['index'],\n",
        "                    'img_name': r['img_name'],\n",
        "                    'img_path': r['img_path'],  # img_path\n",
        "                    'caption': r['caption'],\n",
        "                    'question': r['question'],\n",
        "                    'modality': r['modality'],\n",
        "                    'original_answer': r.get('original_answer', ''),\n",
        "                    'final_score': r['final_score'],\n",
        "                    'medical_quality': r['medical_quality'],\n",
        "                    'clinical_accuracy': r['clinical_accuracy'],\n",
        "                    'structure': r['structure'],\n",
        "                    'image_similarity': r['image_similarity'],\n",
        "                    'question_similarity': r['question_similarity'],\n",
        "                    'detailed_scores': r['detailed_scores']\n",
        "                }\n",
        "                for r in results\n",
        "            ])\n",
        "\n",
        "\n",
        "            display_average_scores(df_results)\n",
        "\n",
        "\n",
        "            top_n = 18\n",
        "            top_indices = df_results['final_score'].nlargest(top_n).index\n",
        "            print(f\"\\nDisplaying top {top_n} highest scoring cases:\")\n",
        "\n",
        "            for i, idx in enumerate(top_indices):\n",
        "                case = df_results.loc[idx].to_dict()\n",
        "                case_rank = i + 1\n",
        "                display_case(case, case['img_path'], f\"Rank {case_rank}: {model_name} Score {case['final_score']:.4f}\", evaluator)\n",
        "\n",
        "\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_dir = '/content/drive/MyDrive/output'\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "            save_df = df_results.drop(columns=['detailed_scores'])\n",
        "            output_path = os.path.join(output_dir, f'blip3_enhanced_evaluation_{timestamp}.csv')\n",
        "            save_df.to_csv(output_path, index=False)\n",
        "            print(f\"\\nResults saved to: {output_path}\")\n",
        "\n",
        "\n",
        "            stats_path = os.path.join(output_dir, f'blip3_enhanced_evaluation_stats_{timestamp}.json')\n",
        "            stats = {\n",
        "                'average_scores': {\n",
        "                    metric: {\n",
        "                        'mean': float(df_results[metric].mean()),\n",
        "                        'std': float(df_results[metric].std()),\n",
        "                        'min': float(df_results[metric].min()),\n",
        "                        'max': float(df_results[metric].max())\n",
        "                    }\n",
        "                    for metric in ['final_score', 'medical_quality', 'clinical_accuracy',\n",
        "                                 'structure', 'image_similarity', 'question_similarity']\n",
        "                },\n",
        "                'sample_count': len(df_results),\n",
        "                'top_case_indices': [int(idx) for idx in top_indices[:5]]\n",
        "            }\n",
        "\n",
        "            with open(stats_path, 'w') as f:\n",
        "                json.dump(stats, f, indent=4)\n",
        "            print(f\"Statistics saved to: {stats_path}\")\n",
        "\n",
        "            return df_results\n",
        "\n",
        "        else:\n",
        "            print(f\"No valid results for {model_name}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "def evaluate_blip3_basic(caption_data, slake_base_dir, evaluator, model_name=\"BLIP3\"):\n",
        "    \"\"\"\n",
        "blip3\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nEvaluating {model_name} results...\")\n",
        "        results = []\n",
        "\n",
        "        for item in tqdm(caption_data, desc=f\"Processing {model_name}\"):\n",
        "            try:\n",
        "\n",
        "                qid = item.get('qid', 0)\n",
        "                img_name = item.get('img_name', '')\n",
        "                question = item.get('question', '')\n",
        "                caption_text = item.get('caption', '')\n",
        "                original_answer = item.get('answer', '')  # answer\n",
        "\n",
        "                if not caption_text:\n",
        "                    print(f\"Warning: Missing caption for qid {qid}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "                img_path = os.path.join(slake_base_dir, 'imgs', img_name)\n",
        "\n",
        "\n",
        "                if not os.path.exists(img_path):\n",
        "                    print(f\"Warning: Image not found at {img_path}\")\n",
        "\n",
        "                    alternative_paths = [\n",
        "                        os.path.join(slake_base_dir, 'img', img_name),\n",
        "                        os.path.join(slake_base_dir, 'images', img_name)\n",
        "                    ]\n",
        "                    for alt_path in alternative_paths:\n",
        "                        if os.path.exists(alt_path):\n",
        "                            img_path = alt_path\n",
        "                            print(f\"Found image at alternative path: {img_path}\")\n",
        "                            break\n",
        "                    else:\n",
        "                        print(f\"Error: Could not find image for {img_name}\")\n",
        "                        continue\n",
        "\n",
        "\n",
        "                image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "                result = evaluator.evaluate_caption(\n",
        "                    image=image,\n",
        "                    caption=caption_text,\n",
        "                    question=question\n",
        "                )\n",
        "\n",
        "                if result:\n",
        "\n",
        "                    result['index'] = qid  # qid as index\n",
        "                    result['img_name'] = img_name\n",
        "                    result['img_path'] = img_path\n",
        "                    result['modality'] = item.get('modality', '')\n",
        "                    result['original_answer'] = original_answer\n",
        "                    results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing item {qid}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "                continue\n",
        "\n",
        "        # DataFrame\n",
        "        if results:\n",
        "            df_results = pd.DataFrame([\n",
        "                {\n",
        "                    'index': r['index'],\n",
        "                    'img_name': r['img_name'],\n",
        "                    'img_path': r['img_path'],\n",
        "                    'caption': r['caption'],\n",
        "                    'question': r['question'],\n",
        "                    'modality': r['modality'],\n",
        "                    'original_answer': r.get('original_answer', ''),\n",
        "                    'final_score': r['final_score'],\n",
        "                    'medical_quality': r['medical_quality'],\n",
        "                    'clinical_accuracy': r['clinical_accuracy'],\n",
        "                    'structure': r['structure'],\n",
        "                    'image_similarity': r['image_similarity'],\n",
        "                    'question_similarity': r['question_similarity'],\n",
        "                    'detailed_scores': r['detailed_scores']\n",
        "                }\n",
        "                for r in results\n",
        "            ])\n",
        "\n",
        "\n",
        "            display_average_scores(df_results)\n",
        "\n",
        "\n",
        "            top_n = 18\n",
        "            top_indices = df_results['final_score'].nlargest(top_n).index\n",
        "            print(f\"\\nDisplaying top {top_n} highest scoring cases:\")\n",
        "\n",
        "            for i, idx in enumerate(top_indices):\n",
        "                case = df_results.loc[idx].to_dict()\n",
        "                case_rank = i + 1\n",
        "                display_case(case, case['img_path'], f\"Rank {case_rank}: {model_name} Score {case['final_score']:.4f}\", evaluator)\n",
        "\n",
        "\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            output_dir = '/content/drive/MyDrive/output'\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "            save_df = df_results.drop(columns=['detailed_scores'])\n",
        "            output_path = os.path.join(output_dir, f'blip3_basic_evaluation_{timestamp}.csv')\n",
        "            save_df.to_csv(output_path, index=False)\n",
        "            print(f\"\\nResults saved to: {output_path}\")\n",
        "\n",
        "\n",
        "            stats_path = os.path.join(output_dir, f'blip3_basic_evaluation_stats_{timestamp}.json')\n",
        "            stats = {\n",
        "                'average_scores': {\n",
        "                    metric: {\n",
        "                        'mean': float(df_results[metric].mean()),\n",
        "                        'std': float(df_results[metric].std()),\n",
        "                        'min': float(df_results[metric].min()),\n",
        "                        'max': float(df_results[metric].max())\n",
        "                    }\n",
        "                    for metric in ['final_score', 'medical_quality', 'clinical_accuracy',\n",
        "                                 'structure', 'image_similarity', 'question_similarity']\n",
        "                },\n",
        "                'sample_count': len(df_results),\n",
        "                'top_case_indices': [int(idx) for idx in top_indices[:5]]\n",
        "            }\n",
        "\n",
        "            with open(stats_path, 'w') as f:\n",
        "                json.dump(stats, f, indent=4)\n",
        "            print(f\"Statistics saved to: {stats_path}\")\n",
        "\n",
        "            return df_results\n",
        "\n",
        "        else:\n",
        "            print(f\"No valid results for {model_name}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error evaluating {model_name}: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "def main_blip3_evaluation():\n",
        "    try:\n",
        "        # 设置路径\n",
        "        slake_base_dir = '/content/drive/MyDrive/slakedataset/Slake1.0'\n",
        "        enhanced_path = '/content/drive/MyDrive/output/enhanced_captions_wholeslake.json'\n",
        "        basic_path = '/content/drive/MyDrive/output/test_captions_slake_blip3_full_20250224_110939.json'\n",
        "\n",
        "        # 初始化评估器\n",
        "        evaluator = MedicalImageCaptionEvaluator()\n",
        "\n",
        "        # BLIP3+Prompt\n",
        "        print(\"\\nProcessing BLIP3+Prompt results...\")\n",
        "        try:\n",
        "            with open(enhanced_path, 'r') as f:\n",
        "                enhanced_data = json.load(f)\n",
        "\n",
        "            if 'results' in enhanced_data:\n",
        "\n",
        "                enhanced_results = enhanced_data['results']\n",
        "            else:\n",
        "\n",
        "                enhanced_results = enhanced_data\n",
        "\n",
        "            print(f\"Loaded {len(enhanced_results)} BLIP3+Prompt captions\")\n",
        "            enhanced_df = evaluate_blip3_enhanced(enhanced_results, slake_base_dir, evaluator, \"BLIP3+Prompt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing BLIP3+Prompt results: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "        # BLIP3\n",
        "        print(\"\\nProcessing BLIP3 results...\")\n",
        "        try:\n",
        "            with open(basic_path, 'r') as f:\n",
        "                basic_data = json.load(f)\n",
        "\n",
        "            if 'results' in basic_data:\n",
        "\n",
        "                basic_results = basic_data['results']\n",
        "            else:\n",
        "\n",
        "                basic_results = basic_data\n",
        "\n",
        "            print(f\"Loaded {len(basic_results)} BLIP3 captions\")\n",
        "            basic_df = evaluate_blip3_basic(basic_results, slake_base_dir, evaluator, \"BLIP3\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing BLIP3 results: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "        print(\"\\nEvaluation complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main_blip3_evaluation: {e}\")\n",
        "        print(traceback.format_exc())"
      ],
      "metadata": {
        "id": "bEnJXD4DjL1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main_blip3_evaluation()"
      ],
      "metadata": {
        "id": "VDSEsLLOk4DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "myt-kMmyk4GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-RKjxj2jL38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
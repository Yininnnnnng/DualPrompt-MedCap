{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#===tag2text with rad========"
      ],
      "metadata": {
        "id": "E9FLpd_zXrZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIgQtdm_pMEl",
        "outputId": "decac9a7-9ef9-4907-f51f-115589145d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import io\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/PhD/Research1/recognize-anything-main')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from ram.models import tag2text\n",
        "    from ram import get_transform\n",
        "    print(\"Successfully imported tag2text from Google Drive\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from Google Drive: {e}\")\n",
        "    print(\"Falling back to cloning the repository in Colab's local filesystem\")\n",
        "    !git clone https://github.com/xinyu1205/recognize-anything.git /content/recognize-anything\n",
        "    %cd /content/recognize-anything\n",
        "    !pip install -e .\n",
        "    %cd /content\n",
        "    sys.path.append('/content/recognize-anything')\n",
        "    from ram.models import tag2text\n",
        "    from ram import get_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBOk0Pxwpc2A",
        "outputId": "313338f7-77f6-47f0-f3bc-867fa66e3e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error importing from Google Drive: No module named 'timm'\n",
            "Falling back to cloning the repository in Colab's local filesystem\n",
            "Cloning into '/content/recognize-anything'...\n",
            "remote: Enumerating objects: 737, done.\u001b[K\n",
            "remote: Counting objects: 100% (447/447), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 737 (delta 310), reused 290 (delta 224), pack-reused 290 (from 1)\u001b[K\n",
            "Receiving objects: 100% (737/737), 27.14 MiB | 24.14 MiB/s, done.\n",
            "Resolving deltas: 100% (397/397), done.\n",
            "/content/recognize-anything\n",
            "Obtaining file:///content/recognize-anything\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clip@ git+https://github.com/openai/CLIP.git (from ram==0.0.1)\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-install-xvwk44zd/clip_818c47fbc2474fb6a792c08071d16b03\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-install-xvwk44zd/clip_818c47fbc2474fb6a792c08071d16b03\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.4.12 (from ram==0.0.1)\n",
            "  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from ram==0.0.1) (4.44.2)\n",
            "Collecting fairscale==0.4.4 (from ram==0.0.1)\n",
            "  Downloading fairscale-0.4.4.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycocoevalcap (from ram==0.0.1)\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ram==0.0.1) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from ram==0.0.1) (0.19.1+cu121)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from ram==0.0.1) (10.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ram==0.0.1) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ram==0.0.1) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->ram==0.0.1) (4.66.5)\n",
            "Collecting ftfy (from clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1)\n",
            "  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap->ram==0.0.1) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip@ git+https://github.com/openai/CLIP.git->ram==0.0.1) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ram==0.0.1) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->ram==0.0.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->ram==0.0.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->ram==0.0.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->ram==0.0.1) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ram==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->ram==0.0.1) (1.16.0)\n",
            "Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairscale, clip\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292831 sha256=8aabdafc4290973cd5cd87f100d8bb9e35258f327a6a94c14a7ae6722bb2f6bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/58/6f/56c57fa8315eb0bcf0287b580c850845be5f116359b809e9f1\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=a3941b7d90f432cd4d42ca1a23fdf37d9a7bc3ba41bc0cae8379f0b6bbc573d5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_xefri2f/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built fairscale clip\n",
            "Installing collected packages: ftfy, fairscale, timm, pycocoevalcap, clip, ram\n",
            "  Running setup.py develop for ram\n",
            "Successfully installed clip-1.0 fairscale-0.4.4 ftfy-6.3.0 pycocoevalcap-1.2 ram-0.0.1 timm-0.4.12\n",
            "/content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
            "/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "global_models = {}\n",
        "\n",
        "def load_models():\n",
        "    if 'tag2text' not in global_models:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "        delete_tag_index = [127, 2961, 3351, 3265, 3338, 3355, 3359]\n",
        "        global_models['tag2text'] = tag2text(pretrained=\"/content/drive/MyDrive/PhD/Research1/recognize-anything-main/pretrained/tag2text_swin_14m.pth\",\n",
        "                                             image_size=384,\n",
        "                                             vit='swin_b',\n",
        "                                             delete_tag_index=delete_tag_index)\n",
        "        global_models['tag2text'].threshold = 0.68  # threshold\n",
        "        global_models['tag2text'].eval()\n",
        "        global_models['tag2text'] = global_models['tag2text'].to(device)\n",
        "        print(\"Tag2Text model loaded successfully\")\n",
        "\n",
        "\n",
        "        global_models['transform'] = get_transform(image_size=384)\n",
        "\n",
        "def process_image(image_data):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    if isinstance(image_data, dict):\n",
        "        image_bytes = image_data['bytes']\n",
        "    elif isinstance(image_data, str):\n",
        "        image_bytes = json.loads(image_data)['bytes']\n",
        "    else:\n",
        "        image_bytes = image_data\n",
        "    image_bytes = bytes(image_bytes)\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "    print(f\"Image size: {image.size}\")\n",
        "\n",
        "    image_tensor = global_models['transform'](image).unsqueeze(0).to(device)\n",
        "\n",
        "    try:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            result = global_models['tag2text'].generate(image_tensor, return_tag_predict=True)\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            captions, tags = result\n",
        "            caption = captions[0] if captions else \"No caption generated\"\n",
        "        else:\n",
        "            tags = result[0] if isinstance(result, list) else \"No tags generated\"\n",
        "            caption = \"No caption generated\"\n",
        "\n",
        "\n",
        "        if isinstance(tags, str):\n",
        "            tags = tags.split('|')\n",
        "\n",
        "        print(f\"Tag2Text generated tags: {tags}\")\n",
        "        print(f\"Tag2Text generated caption: {caption}\")\n",
        "\n",
        "        return tags, caption\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        return [], \"Error generating caption\"\n"
      ],
      "metadata": {
        "id": "Xh9RfE3zpc4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    load_models()\n",
        "\n",
        "\n",
        "    input_parquet = '/content/drive/MyDrive/PhD/Research1/RADdataset/test-00000-of-00001-e5bc3d208bb4deeb.parquet'\n",
        "    output_json = '/content/drive/MyDrive/PhD/Research1/RADdataset/processed_data.json'\n",
        "\n",
        "\n",
        "    df = pd.read_parquet(input_parquet)\n",
        "    print(f\"Loaded {len(df)} rows from the parquet file.\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing images\"):\n",
        "        image_data = row['image']\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "\n",
        "        print(f\"\\nProcessing image {index}\")\n",
        "        print(f\"Question: {question}\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            tags, tag2text_caption = process_image(image_data)\n",
        "\n",
        "\n",
        "            result = {\n",
        "                \"index\": index,\n",
        "                \"question\": question,\n",
        "                \"original_answer\": answer,\n",
        "                \"tag2text_caption\": tag2text_caption,\n",
        "                \"tags\": tags,\n",
        "                \"blip3_caption\": None\n",
        "            }\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {index}: {e}\")\n",
        "\n",
        "            results.append({\n",
        "                \"index\": index,\n",
        "                \"question\": question,\n",
        "                \"original_answer\": answer,\n",
        "                \"tag2text_caption\": \"Error generating caption\",\n",
        "                \"tags\": [],\n",
        "                \"blip3_caption\": None\n",
        "            })\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    with open(output_json, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"Processing complete. Results saved to {output_json}\")"
      ],
      "metadata": {
        "id": "plhMqhaQpc6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Eqc-dlh7pc86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09xrh8yVcXV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===tag2text with slake========"
      ],
      "metadata": {
        "id": "Hwnfu0SkpPFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')#\n",
        "import io\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n"
      ],
      "metadata": {
        "id": "dGWAM-JypPHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e20717-1746-42e1-ea6d-1db627db2cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sys.path.append('/content/drive/MyDrive/PhD/Research1/recognize-anything-main')\n",
        "try:\n",
        "\n",
        "    import importlib\n",
        "    if not importlib.util.find_spec(\"fairscale\"):\n",
        "        print(\"Installing fairscale...\")\n",
        "        !pip install fairscale\n",
        "    if not importlib.util.find_spec(\"timm\"):\n",
        "        print(\"Installing timm...\")\n",
        "        !pip install timm\n",
        "    if not importlib.util.find_spec(\"transformers\"):\n",
        "        print(\"Installing transformers...\")\n",
        "        !pip install transformers\n",
        "\n",
        "    from ram.models import tag2text\n",
        "    from ram import get_transform\n",
        "    print(\"Successfully imported tag2text from Google Drive\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing from Google Drive: {e}\")\n",
        "    print(\"Falling back to cloning the repository in Colab's local filesystem\")\n",
        "    !git clone https://github.com/xinyu1205/recognize-anything.git /content/recognize-anything\n",
        "    %cd /content/recognize-anything\n",
        "\n",
        "    !pip install fairscale timm transformers\n",
        "    !pip install -e .\n",
        "    %cd /content\n",
        "    sys.path.append('/content/recognize-anything')\n",
        "    from ram.models import tag2text\n",
        "    from ram import get_transform\n",
        "\n",
        "\n",
        "global_models = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twsue4QrUG1p",
        "outputId": "ead7ac29-e3e6-445c-ddec-98598b367f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/hub.py:4: FutureWarning: Importing from timm.models.hub is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully imported tag2text from Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models():\n",
        "    if 'tag2text' not in global_models:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "        delete_tag_index = [127, 2961, 3351, 3265, 3338, 3355, 3359]\n",
        "        global_models['tag2text'] = tag2text(pretrained=\"/content/drive/MyDrive/PhD/Research1/recognize-anything-main/pretrained/tag2text_swin_14m.pth\",\n",
        "                                           image_size=384,\n",
        "                                           vit='swin_b',\n",
        "                                           delete_tag_index=delete_tag_index)\n",
        "        global_models['tag2text'].threshold = 0.68\n",
        "        global_models['tag2text'].eval()\n",
        "        global_models['tag2text'] = global_models['tag2text'].to(device)\n",
        "        print(\"Tag2Text model loaded successfully\")\n",
        "\n",
        "\n",
        "        global_models['transform'] = get_transform(image_size=384)\n",
        "\n",
        "def process_image(image_path):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    try:\n",
        "        print(f\"Loading image from: {image_path}\")\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"ERROR: Image file not found: {image_path}\")\n",
        "            return [], \"Error: Image file not found\"\n",
        "\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        print(f\"Image successfully loaded. Size: {image.size}\")\n",
        "\n",
        "\n",
        "        print(\"Applying image transformation...\")\n",
        "        image_tensor = global_models['transform'](image).unsqueeze(0).to(device)\n",
        "        print(f\"Image transformed to tensor of shape: {image_tensor.shape}\")\n",
        "\n",
        "\n",
        "        print(\"Running Tag2Text model inference...\")\n",
        "        with torch.no_grad():\n",
        "            result = global_models['tag2text'].generate(image_tensor, return_tag_predict=True)\n",
        "        print(\"Tag2Text inference completed\")\n",
        "\n",
        "        if isinstance(result, tuple) and len(result) == 2:\n",
        "            captions, tags = result\n",
        "            caption = captions[0] if captions else \"No caption generated\"\n",
        "            print(f\"Got caption: {caption}\")\n",
        "        else:\n",
        "            tags = result[0] if isinstance(result, list) else \"No tags generated\"\n",
        "            caption = \"No caption generated\"\n",
        "            print(\"No caption in result, only tags\")\n",
        "\n",
        "\n",
        "        if isinstance(tags, str):\n",
        "            tags = tags.split('|')\n",
        "\n",
        "        print(f\"Tag2Text generated tags: {tags}\")\n",
        "        print(f\"Tag2Text generated caption: {caption}\")\n",
        "\n",
        "        return tags, caption\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        print(traceback.format_exc())\n",
        "        return [], f\"Error generating caption: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "\n",
        "    load_models()\n",
        "\n",
        "\n",
        "    slake_base_dir = '/content/drive/MyDrive/PhD/Research1/slakedataset/Slake1.0'\n",
        "    json_path = os.path.join(slake_base_dir, 'test.json')\n",
        "    output_dir = '/content/drive/MyDrive/PhD/Research1/output'\n",
        "    output_json = os.path.join(output_dir, 'slake_tag2text_results.json')\n",
        "\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "\n",
        "    english_samples = [item for item in data if item.get('q_lang') == 'en']\n",
        "    print(f\"Loaded {len(english_samples)} English samples from test.json\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "\n",
        "    temp_output_json = os.path.join(output_dir, 'slake_tag2text_results_temp.json')\n",
        "\n",
        "\n",
        "    for idx, sample in tqdm(enumerate(english_samples), total=len(english_samples), desc=\"Processing images\"):\n",
        "        img_name = sample.get('img_name', '')\n",
        "        img_id = sample.get('img_id', '')\n",
        "        question = sample.get('question', '')\n",
        "        answer = sample.get('answer', '')\n",
        "        modality = sample.get('modality', '')\n",
        "\n",
        "\n",
        "        img_path = os.path.join(slake_base_dir, 'imgs', img_name)\n",
        "\n",
        "        print(f\"\\nProcessing image {idx} (ID: {img_id})\")\n",
        "        print(f\"Image path: {img_path}\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Modality: {modality}\")\n",
        "\n",
        "        try:\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                print(f\"WARNING: Image file not found at {img_path}\")\n",
        "\n",
        "                alternative_paths = [\n",
        "                    os.path.join(slake_base_dir, 'img', img_name),\n",
        "                    os.path.join(slake_base_dir, 'images', img_name)\n",
        "                ]\n",
        "                for alt_path in alternative_paths:\n",
        "                    if os.path.exists(alt_path):\n",
        "                        img_path = alt_path\n",
        "                        print(f\"Found image at alternative path: {img_path}\")\n",
        "                        break\n",
        "\n",
        "\n",
        "            tags, tag2text_caption = process_image(img_path)\n",
        "\n",
        "\n",
        "            result = {\n",
        "                \"id\": idx,\n",
        "                \"img_id\": img_id,\n",
        "                \"img_name\": img_name,\n",
        "                \"question\": question,\n",
        "                \"original_answer\": answer,\n",
        "                \"modality\": modality,\n",
        "                \"tag2text_caption\": tag2text_caption,\n",
        "                \"tags\": tags,\n",
        "                \"blip3_caption\": None\n",
        "            }\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"Error processing sample {idx}, image {img_name}: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "            results.append({\n",
        "                \"id\": idx,\n",
        "                \"img_id\": img_id,\n",
        "                \"img_name\": img_name,\n",
        "                \"question\": question,\n",
        "                \"original_answer\": answer,\n",
        "                \"modality\": modality,\n",
        "                \"tag2text_caption\": f\"Error generating caption: {str(e)}\",\n",
        "                \"tags\": [],\n",
        "                \"blip3_caption\": None\n",
        "            })\n",
        "\n",
        "\n",
        "        if (idx + 1) % 10 == 0:\n",
        "            try:\n",
        "                with open(temp_output_json, 'w') as f:\n",
        "                    json.dump(results, f, indent=2)\n",
        "                print(f\"Temporary results saved to {temp_output_json} after processing {idx+1} samples\")\n",
        "            except Exception as save_error:\n",
        "                print(f\"Error saving temporary results: {save_error}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(output_json, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Processing complete. Results saved to {output_json}\")\n",
        "    except Exception as save_error:\n",
        "        print(f\"Error saving final results: {save_error}\")\n",
        "\n",
        "        backup_output = os.path.join('/content', 'slake_tag2text_results_backup.json')\n",
        "        with open(backup_output, 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results saved to backup location: {backup_output}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "tPjmq8N-UG7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l0kuaqhJUG-J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
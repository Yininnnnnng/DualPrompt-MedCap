
# DualPrompt-MedCap

This repository contains the official implementation of the MICCAI 2025 paper:

**"DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning"**

## üìå Overview

DualPrompt-MedCap is a dual-prompt learning framework for medical image captioning that introduces:
- A **semi-supervised modality classifier** to generate **modality prompts**.
- A **question-driven prompt extractor** embedded within a BLIP-3-based captioning pipeline.
- An innovative **ground truth-free evaluation framework** for clinical assessment.

The codebase supports experiments on both **SLAKE** and **IU X-Ray (RAD)** datasets.

---

## üß≠ Project Structure

### 1. `modality_classifier/`
Train a semi-supervised image modality classifier (e.g., CT, X-ray, MRI). This module:
- Uses ResNet + Medical Modality Attention
- Saves model weights as `.pth` files
- Required for downstream captioning

Includes ablation studies with alternative backbone configurations.

### 2. `caption_generation/`
BLIP-3-based generation model with dual prompt mechanism:
- **Modality prompt**: dynamically generated by loading `.pth` from the modality classifier
- **Question prompt**: auto-constructed via PubMedBERT similarity scoring (no separate module)

Both prompts guide the BLIP-3 model during zero-shot caption generation.

### 3. `evaluation/`
Reference-free evaluation metrics include:
- `S_medical`: UMLS terminology density and diversity
- `S_clinical`: dictionary-based correctness across 4 clinical axes
- `S_structure`: logical structure, multi-sentence coherence, modality mention

These metrics are fully automated, using string-matching and NLP parsing‚Äîno human annotation required.

### 4. `baselines/`
Implements and evaluates baseline methods:
- BLIP-2 (zero-shot)
- BLIP-3 without prompt
- Tag2Text

Each is evaluated under identical settings for fair comparison.

---

## üìÇ Dataset Handling

- Both **SLAKE** and **RAD** datasets are supported.
- Dataset-specific pre-processing (e.g., augmentation) is embedded in the training and evaluation scripts.
- Dataset selection is controlled via internal flags or config settings.

---

## üöÄ Execution Flow

1. **Train the modality classifier**:
   ```
   cd modality_classifier
   python train.py
   ```
   Output: `model_xxx.pth`

2. **Generate captions with BLIP-3**:
   ```
   cd caption_generation
   python generate.py --load_modality_pth ../modality_classifier/model_xxx.pth
   ```

3. **Evaluate captions**:
   ```
   cd evaluation
   python eval_metrics.py --pred_file ../caption_generation/output.json
   ```

---

## üîì Model Checkpoints

We provide the trained `.pth` model files used for modality prompt generation (available in `checkpoints/`). These can be used to directly reproduce the results.

---

## üìÑ Citation

If you use this code, please cite:

```
@inproceedings{zhao2025dualprompt,
  title={DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning},
  author={Zhao, Yining and et al.},
  booktitle={MICCAI},
  year={2025}
}
```

---

## üõ†Ô∏è License

This repository is released under the MIT license.

For questions or issues, please contact the corresponding author.

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dolBXSx-g-xb"
      },
      "outputs": [],
      "source": [
        "# environment\n",
        "!pip uninstall -y transformers numpy torch torchvision torchaudio\n",
        "\n",
        "\n",
        "!pip install numpy==1.26.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install open_clip_torch==2.24.0\n",
        "!pip install einops\n",
        "!pip install einops-exts\n",
        "!pip install transformers==4.41.1"
      ],
      "metadata": {
        "id": "x_pVzgjYmGY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoImageProcessor, AutoModel, StoppingCriteria\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import torchvision.transforms as transforms\n",
        "from datetime import datetime\n",
        "import torchvision.models as models\n",
        "from collections import defaultdict\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import traceback\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 挂载Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctoknQxdhASu",
        "outputId": "5a86343d-bd7b-4ef9-a293-75fc65dd4228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EosListStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, eos_sequence = [32007]):\n",
        "        self.eos_sequence = eos_sequence\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        last_ids = input_ids[:,-len(self.eos_sequence):].tolist()\n",
        "        return self.eos_sequence in last_ids"
      ],
      "metadata": {
        "id": "O36oFu9-hAU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MedicalModalityAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.anatomy_attention = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels//2, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels//2, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.texture_attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels//16, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels//16, in_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.multi_scale = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, in_channels//4, 3, padding=1, dilation=1),\n",
        "            nn.Conv2d(in_channels, in_channels//4, 3, padding=2, dilation=2),\n",
        "            nn.Conv2d(in_channels, in_channels//4, 3, padding=4, dilation=4)\n",
        "        ])\n",
        "\n",
        "        self.channel_adjust = nn.Conv2d(in_channels//4*3, in_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        anatomy_weight = self.anatomy_attention(x)\n",
        "        texture_weight = self.texture_attention(x)\n",
        "\n",
        "        multi_scale_features = []\n",
        "        for conv in self.multi_scale:\n",
        "            multi_scale_features.append(conv(x))\n",
        "        multi_scale_feat = torch.cat(multi_scale_features, dim=1)\n",
        "        multi_scale_feat = self.channel_adjust(multi_scale_feat)\n",
        "\n",
        "        enhanced = x * anatomy_weight * texture_weight\n",
        "        return enhanced + multi_scale_feat\n",
        "\n",
        "class ModalityClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "        self.attention1 = MedicalModalityAttention(256)\n",
        "        self.attention2 = MedicalModalityAttention(512)\n",
        "        self.attention3 = MedicalModalityAttention(1024)\n",
        "        self.attention4 = MedicalModalityAttention(2048)\n",
        "\n",
        "\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.modality_head = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.attention1(self.backbone.layer1(x))\n",
        "        x = self.attention2(self.backbone.layer2(x))\n",
        "        x = self.attention3(self.backbone.layer3(x))\n",
        "        x = self.attention4(self.backbone.layer4(x))\n",
        "\n",
        "        x = self.backbone.avgpool(x)\n",
        "        features = torch.flatten(x, 1)\n",
        "        x = self.modality_head(features)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "6sF2yBqwhAW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedQuestionAnalyzer:\n",
        "    def __init__(self, question_model, question_tokenizer):\n",
        "        self.pubmed_bert = question_model\n",
        "        self.tokenizer = question_tokenizer\n",
        "\n",
        "        self.medical_concepts = {\n",
        "            'anatomy': [\"lung\", \"heart\", \"liver\", \"brain\", \"chest\", \"spine\"],\n",
        "            'pathology': [\"tumor\", \"nodule\", \"mass\", \"lesion\", \"cancer\", \"abnormal\"],\n",
        "            'location': [\"upper\", \"lower\", \"left\", \"right\", \"central\"],\n",
        "            'comparison': [\"larger\", \"smaller\", \"normal size\", \"biggest\"]\n",
        "        }\n",
        "\n",
        "        self.concept_embeddings = self._initialize_concept_embeddings()\n",
        "\n",
        "    def _initialize_concept_embeddings(self):\n",
        "        concept_embeddings = {}\n",
        "        for concept_type, terms in self.medical_concepts.items():\n",
        "\n",
        "            term_encodings = self.tokenizer(\n",
        "                terms,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                try:\n",
        "                    outputs = self.pubmed_bert(**term_encodings)\n",
        "                except Exception as e:\n",
        "                    print(f\"Using fallback API for PubMedBERT: {e}\")\n",
        "                    outputs = self.pubmed_bert(term_encodings.input_ids,\n",
        "                                               attention_mask=term_encodings.attention_mask)\n",
        "\n",
        "                embeddings = outputs.last_hidden_state[:, 0, :]  # [num_terms, hidden_size]\n",
        "                concept_embeddings[concept_type] = embeddings\n",
        "\n",
        "        return concept_embeddings\n",
        "\n",
        "    def analyze_question_type(self, question: str) -> Dict[str, float]:\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            question,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            try:\n",
        "                outputs = self.pubmed_bert(**inputs)\n",
        "            except Exception as e:\n",
        "                print(f\"Using fallback API for PubMedBERT: {e}\")\n",
        "                outputs = self.pubmed_bert(inputs.input_ids,\n",
        "                                           attention_mask=inputs.attention_mask)\n",
        "\n",
        "            question_embedding = outputs.last_hidden_state[:, 0, :]  # [1, hidden_size]\n",
        "\n",
        "\n",
        "        concept_scores = {}\n",
        "        for concept_type, concept_emb in self.concept_embeddings.items():\n",
        "            similarities = torch.cosine_similarity(\n",
        "                question_embedding.unsqueeze(1),  # [1, 1, hidden_size]\n",
        "                concept_emb,                      # [num_terms, hidden_size]\n",
        "                dim=2\n",
        "            )\n",
        "            concept_scores[concept_type] = similarities.mean().item()\n",
        "\n",
        "        return concept_scores\n",
        "\n",
        "    def get_question_focus(self, question: str) -> str:\n",
        "        concept_scores = self.analyze_question_type(question)\n",
        "        primary_focus = max(concept_scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "        focus_templates = {\n",
        "            'anatomy': 'examine anatomical structures and their relationships',\n",
        "            'pathology': 'detect any pathological changes or abnormalities',\n",
        "            'location': 'focus on spatial relationships and positioning',\n",
        "            'comparison': 'compare sizes and relative appearances'\n",
        "        }\n",
        "\n",
        "        base_focus = focus_templates[primary_focus]\n",
        "\n",
        "\n",
        "        relevant_concepts = [k for k, v in concept_scores.items()\n",
        "                            if v > 0.5 and k != primary_focus]\n",
        "        if relevant_concepts:\n",
        "            additional_focuses = [focus_templates[c] for c in relevant_concepts]\n",
        "            base_focus += f\", particularly {', '.join(additional_focuses)}\"\n",
        "\n",
        "        return base_focus"
      ],
      "metadata": {
        "id": "E9UIlAKChAZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedEvaluator:\n",
        "    def __init__(self):\n",
        "        self.reset_stats()\n",
        "\n",
        "    def reset_stats(self):\n",
        "        \"\"\"restart\"\"\"\n",
        "        self.stats = {\n",
        "            'total_samples': 0,\n",
        "            'correct_predictions': 0,\n",
        "            'confusion_matrix': defaultdict(lambda: defaultdict(int)),\n",
        "            'modality_errors': [],\n",
        "            'per_modality_stats': defaultdict(lambda: {'total': 0, 'correct': 0}),\n",
        "            'confidence_distribution': {\n",
        "                'high': {'count': 0, 'correct': 0},  # >= 0.8\n",
        "                'low': {'count': 0, 'correct': 0}    # < 0.8\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def update(self, true_modality: str, predicted_modality: str,\n",
        "               confidence: float, img_name: str, caption: str = None):\n",
        "        \"\"\"restart\"\"\"\n",
        "        true_mod = true_modality.upper()\n",
        "        pred_mod = predicted_modality.upper()\n",
        "\n",
        "\n",
        "        self.stats['total_samples'] += 1\n",
        "        self.stats['confusion_matrix'][true_mod][pred_mod] += 1\n",
        "        self.stats['per_modality_stats'][true_mod]['total'] += 1\n",
        "\n",
        "\n",
        "        conf_category = 'high' if confidence >= 0.8 else 'low'\n",
        "        self.stats['confidence_distribution'][conf_category]['count'] += 1\n",
        "\n",
        "\n",
        "        if true_mod == pred_mod:\n",
        "            self.stats['correct_predictions'] += 1\n",
        "            self.stats['per_modality_stats'][true_mod]['correct'] += 1\n",
        "            self.stats['confidence_distribution'][conf_category]['correct'] += 1\n",
        "        else:\n",
        "            self.stats['modality_errors'].append({\n",
        "                'image_name': img_name,\n",
        "                'true_modality': true_mod,\n",
        "                'predicted_modality': pred_mod,\n",
        "                'confidence': confidence,\n",
        "                'caption': caption\n",
        "            })\n",
        "\n",
        "    def print_evaluation_results(self):\n",
        "        \"\"\"简明打印关键评估结果\"\"\"\n",
        "        print(\"\\n=== Modality Classification Quick Check ===\")\n",
        "\n",
        "\n",
        "        acc = self.stats['correct_predictions'] / self.stats['total_samples']\n",
        "        print(f\"Overall Accuracy: {acc:.2%}\")\n",
        "\n",
        "\n",
        "        print(\"\\nPer Modality Accuracy:\")\n",
        "        for modality, stats in self.stats['per_modality_stats'].items():\n",
        "            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
        "            print(f\"{modality}: {acc:.2%} ({stats['correct']}/{stats['total']})\")\n",
        "\n",
        "\n",
        "        print(\"\\nAttention Required:\")\n",
        "        for modality, stats in self.stats['per_modality_stats'].items():\n",
        "            acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
        "            if acc < 0.8:\n",
        "                print(f\"- {modality} needs improvement: {acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "wVq4aOJ7hAbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def apply_prompt_template(question: str, clinical_focus: str, modality_info: str) -> str:\n",
        "    return f\"\"\"\n",
        "    <|system|>\n",
        "    You are a radiologist examining this {modality_info} image. Question: {question}\n",
        "    Describe thoroughly: {clinical_focus}.\n",
        "    First describe any abnormal density patterns or structural variations, then detail normal findings if present.\n",
        "    <|end|>\n",
        "\n",
        "    <|user|>\n",
        "    <image>\n",
        "    Describe all findings systematically.\n",
        "    <|end|>\n",
        "\n",
        "    <|assistant|>\n",
        "    \"\"\"\n",
        "\n",
        "def generate_enhanced_caption(image, item):\n",
        "    \"\"\"enhaced\"\"\"\n",
        "    try:\n",
        "        # 1. Modality\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        img_tensor = transform(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = modality_classifier(img_tensor)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            confidence, pred_idx = probs.max(1)\n",
        "            predicted_modality = ['MRI', 'CT', 'X-ray'][pred_idx.item()]\n",
        "\n",
        "        # 2.  based confidence with modality\n",
        "        modality_info = f\"likely a {predicted_modality}\" if confidence < 0.8 else f\"a {predicted_modality}\"\n",
        "\n",
        "        # 3. question_analyzer with focus\n",
        "        clinical_focus = question_analyzer.get_question_focus(item['question'])\n",
        "\n",
        "        # 4. prompt\n",
        "        prompt = apply_prompt_template(\n",
        "            question=item['question'],\n",
        "            modality_info=modality_info,\n",
        "            clinical_focus=clinical_focus\n",
        "        )\n",
        "\n",
        "        # 5. input modality\n",
        "        inputs = image_processor(\n",
        "            [image],\n",
        "            return_tensors=\"pt\",\n",
        "            image_aspect_ratio='anyres'\n",
        "        ).to(device)\n",
        "\n",
        "        language_inputs = tokenizer(\n",
        "            [prompt],\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        inputs.update(language_inputs)\n",
        "\n",
        "        # 6. generate caption\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                image_size=[image.size],\n",
        "                do_sample=True,\n",
        "                max_new_tokens=150,\n",
        "                num_beams=5,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                stopping_criteria=[EosListStoppingCriteria()],\n",
        "            )\n",
        "\n",
        "\n",
        "        caption = tokenizer.decode(\n",
        "            generated_ids[0],\n",
        "            skip_special_tokens=True\n",
        "        ).split(\"<|end|>\")[0].strip()\n",
        "\n",
        "\n",
        "        if confidence < 0.8:\n",
        "            words = caption.split()\n",
        "            certainty_words = [\"shows\", \"demonstrates\", \"reveals\", \"indicates\", \"confirms\"]\n",
        "            for i, word in enumerate(words):\n",
        "                if word.lower() in certainty_words:\n",
        "                    words[i] = \"may \" + word\n",
        "                    break\n",
        "            caption = \" \".join(words)\n",
        "\n",
        "        # 8. evaluator\n",
        "        sample_evaluator = EnhancedEvaluator()\n",
        "        sample_evaluator.update(\n",
        "            item['modality'],\n",
        "            predicted_modality,\n",
        "            confidence.item(),\n",
        "            item['img_name'],\n",
        "            caption\n",
        "        )\n",
        "\n",
        "\n",
        "        return caption, predicted_modality, confidence.item(), prompt, sample_evaluator\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate_enhanced_caption for image {item.get('img_name', 'unknown')}: {str(e)}\")\n",
        "        print(\"Full traceback:\")\n",
        "        print(traceback.format_exc())\n",
        "        return \"\", \"unknown\", 0.0, \"\", EnhancedEvaluator()"
      ],
      "metadata": {
        "id": "TqthdxpXhAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_set_captions():\n",
        "    \"\"\"formal generation\"\"\"\n",
        "    print(f\"\\nDebug - Using device: {device}\")\n",
        "\n",
        "    # path\n",
        "    slake_path = \"/content/drive/MyDrive/PhD/Research1/slakedataset/Slake1.0\"\n",
        "    output_file = '/content/drive/MyDrive/PhD/Research1/output/enhanced_captions_train.json'\n",
        "\n",
        "    # test.json - train.json\n",
        "    print(\"\\nLoading training data...\")\n",
        "    try:\n",
        "        with open(os.path.join(slake_path, 'train.json'), 'r') as f:\n",
        "            train_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading training data: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "    # english\n",
        "    english_data = [item for item in train_data if item['q_lang'] == 'en']\n",
        "    print(f\"\\nTesting on {len(english_data)} English samples from training set\")\n",
        "\n",
        "\n",
        "    evaluator = EnhancedEvaluator()\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for idx, item in enumerate(tqdm(english_data, desc=\"Processing training samples\",\n",
        "                                  total=len(english_data))):\n",
        "        try:\n",
        "\n",
        "            img_path = os.path.join(slake_path, 'imgs', item['img_name'])\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "\n",
        "            caption, predicted_modality, confidence, enhanced_prompt, sample_evaluator = generate_enhanced_caption(\n",
        "                image, item\n",
        "            )\n",
        "\n",
        "\n",
        "            evaluator.update(\n",
        "                item['modality'],\n",
        "                predicted_modality,\n",
        "                confidence,\n",
        "                item['img_name'],\n",
        "                caption\n",
        "            )\n",
        "\n",
        "\n",
        "            result = {\n",
        "                'qid': item['qid'],\n",
        "                'img_name': item['img_name'],\n",
        "                'true_modality': item['modality'],\n",
        "                'predicted_modality': predicted_modality,\n",
        "                'confidence': confidence,\n",
        "                'question': item['question'],\n",
        "                'enhanced_prompt': enhanced_prompt,\n",
        "                'generated_caption': caption,\n",
        "                'original_answer': item['answer']\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"\\nProcessed {idx + 1}/{len(english_data)} samples\")\n",
        "                print(\"\\nIntermediate Evaluation Results:\")\n",
        "                evaluator.print_evaluation_results()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError processing image {item['img_name']}: {str(e)}\")\n",
        "            print(\"Full traceback:\")\n",
        "            print(traceback.format_exc())\n",
        "            continue\n",
        "\n",
        "\n",
        "    print(\"\\n=== Final Evaluation Results ===\")\n",
        "    evaluator.print_evaluation_results()\n",
        "\n",
        "\n",
        "    try:\n",
        "        output_data = {\n",
        "            'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'results': results,\n",
        "            'evaluation_stats': evaluator.stats,\n",
        "            'run_summary': {\n",
        "                'total_samples': len(english_data),\n",
        "                'processed_samples': len(results),\n",
        "                'success_rate': len(results) / len(english_data)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            json.dump(output_data, f, indent=2)\n",
        "        print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {str(e)}\")\n",
        "        print(traceback.format_exc())\n",
        "\n",
        "    return output_file, evaluator"
      ],
      "metadata": {
        "id": "gZIqRbcF3oBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading models following official documentation...\")\n",
        "\n",
        "\n",
        "model_name = \"Salesforce/xgen-mm-phi3-mini-instruct-r-v1\"\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        "    legacy=False\n",
        ")\n",
        "image_processor = AutoImageProcessor.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = model.update_special_tokens(tokenizer)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "question_model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "question_tokenizer = AutoTokenizer.from_pretrained(question_model_name)\n",
        "question_model = AutoModel.from_pretrained(question_model_name).to(device)\n",
        "\n",
        "\n",
        "question_analyzer = EnhancedQuestionAnalyzer(question_model, question_tokenizer)\n",
        "\n",
        "\n",
        "modality_classifier = ModalityClassifier(num_classes=3).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/PhD/Research1/modelparasave/slake_modality_model_improved.pth',\n",
        "                       map_location=device)\n",
        "modality_classifier.load_state_dict(checkpoint['model_state_dict'])\n",
        "modality_classifier.eval()\n"
      ],
      "metadata": {
        "id": "6tOl4m_XoXOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting enhanced caption generation for training set...\")\n",
        "    output_file, evaluator = generate_train_set_captions()"
      ],
      "metadata": {
        "id": "wU0fSira4fSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hbFlqMWPoXTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}